{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauldebayan/AlgoTrading/blob/main/SpectaGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f66445c-d883-440c-a8de-a4815438f483",
      "metadata": {
        "tags": [],
        "id": "8f66445c-d883-440c-a8de-a4815438f483"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/pauldebayan/SpectaGen/refs/heads/main/labels.csv\n",
        "!wget https://github.com/pauldebayan/SpectaGen/raw/refs/heads/main/spectacle_dataset.zip\n",
        "!unzip spectacle_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3920d6-851a-463c-aa73-e0db0661b045",
      "metadata": {
        "tags": [],
        "id": "5e3920d6-851a-463c-aa73-e0db0661b045"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9c1700-d570-4074-b791-b547e3b74898",
      "metadata": {
        "tags": [],
        "id": "5f9c1700-d570-4074-b791-b547e3b74898"
      },
      "outputs": [],
      "source": [
        "def ImagesToDelete():\n",
        "    # To check all the images is of same shape\n",
        "    channel, height, width = 3, 512, 512\n",
        "\n",
        "    imagesToDelete = []\n",
        "\n",
        "    for i in range(1, 1000):\n",
        "        img = read_image(f'./spectacle_dataset/specs{(i+1)}.jpg')\n",
        "\n",
        "        if(img.shape[0] != channel or img.shape[1] != height or img.shape[2] != width):\n",
        "            imagesToDelete.append(f'specs{(i+1)}.jpg')\n",
        "\n",
        "    # We need to delete this images as they do not have the desired shape - [3, 256, 512]\n",
        "    return imagesToDelete\n",
        "\n",
        "print(ImagesToDelete())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d06f688-010d-46d0-9fa3-4b002eb8a47b",
      "metadata": {
        "tags": [],
        "id": "5d06f688-010d-46d0-9fa3-4b002eb8a47b"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available and move model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89f7250-cd7b-4451-b9cb-53b81be6c3d6",
      "metadata": {
        "tags": [],
        "id": "b89f7250-cd7b-4451-b9cb-53b81be6c3d6"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 20, 2)\n",
        "        self.conv2 = nn.Conv2d(20, 10, 2)\n",
        "        self.conv3 = nn.Conv2d(10, 10, 2)\n",
        "\n",
        "        self.activ = nn.LeakyReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(39690, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256,64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.pool(self.activ(self.conv1(x)))\n",
        "        x = self.pool(self.activ(self.conv2(x)))\n",
        "        x = self.pool(self.activ(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_stack(x)\n",
        "        return x\n",
        "\n",
        "        #return x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b24aa5c-a106-48f9-bdcf-48ccd7fc690d",
      "metadata": {
        "tags": [],
        "id": "0b24aa5c-a106-48f9-bdcf-48ccd7fc690d"
      },
      "outputs": [],
      "source": [
        "class Genearator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gen_sequence = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(128, 512, 3, stride=3),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, 5, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 256, 3, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 256, 3, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 3, 4, stride=4),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 128, 2, 2)  # [batch_size, channels, height, width]\n",
        "        x = self.gen_sequence(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1198f0f6-0bc0-43c1-9b6e-6ad32a265d69",
      "metadata": {
        "tags": [],
        "id": "1198f0f6-0bc0-43c1-9b6e-6ad32a265d69"
      },
      "outputs": [],
      "source": [
        "generator = Genearator()\n",
        "generator = generator.to(device)\n",
        "discriminator = Discriminator()\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "print(discriminator)\n",
        "print(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074f609e-bb2d-443d-bcd2-c3af025f4b37",
      "metadata": {
        "tags": [],
        "id": "074f609e-bb2d-443d-bcd2-c3af025f4b37"
      },
      "outputs": [],
      "source": [
        "#generator.load_state_dict(torch.load('generator.pt'))\n",
        "#discriminator.load_state_dict(torch.load('discriminator.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57648597-a30c-4cae-8193-7b44d7acc800",
      "metadata": {
        "tags": [],
        "id": "57648597-a30c-4cae-8193-7b44d7acc800"
      },
      "outputs": [],
      "source": [
        "# To get the input of the linear layer in Discriminator\n",
        "\n",
        "img = read_image('./spectacle_dataset/specs1.jpg')\n",
        "print(img.shape)\n",
        "print(discriminator(img.to(device).float()))\n",
        "#39690\n",
        "#Will cause error because of batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9501821d-ea8f-4b34-9bdf-2321d2c5968b",
      "metadata": {
        "tags": [],
        "id": "9501821d-ea8f-4b34-9bdf-2321d2c5968b"
      },
      "outputs": [],
      "source": [
        "def generateNoise():\n",
        "    noise = torch.randn(1, 512)\n",
        "    return noise\n",
        "\n",
        "# Conv2d: (N-K+1)/S\n",
        "# ConvTranspose2d: (N-1)*S+K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c1bbac-0a1e-40e0-8a24-9198f5b34219",
      "metadata": {
        "tags": [],
        "id": "40c1bbac-0a1e-40e0-8a24-9198f5b34219"
      },
      "outputs": [],
      "source": [
        "noise = generateNoise()\n",
        "noise = noise.to(device)\n",
        "img = generator(noise)\n",
        "\n",
        "img = img.to(device)\n",
        "\n",
        "print(img.shape)\n",
        "plt.imshow(img.cpu().squeeze().detach().permute(1, 2, 0))\n",
        "\n",
        "print(f\"Fake image shape: {img.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1986d037-60f9-4af6-85af-c45614b079fb",
      "metadata": {
        "tags": [],
        "id": "1986d037-60f9-4af6-85af-c45614b079fb"
      },
      "outputs": [],
      "source": [
        "# Real Images - set dataloader\n",
        "class SpectDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3b4ed6-913f-4d84-be4a-14743710e433",
      "metadata": {
        "tags": [],
        "id": "fa3b4ed6-913f-4d84-be4a-14743710e433"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = SpectDataset(img_dir = 'spectacle_dataset',\n",
        "                             annotations_file = 'labels.csv',\n",
        "                             transform = transform)\n",
        "\n",
        "batch_size = 5\n",
        "dl = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b667b32e-90d9-42b9-b5ff-456fbc04f522",
      "metadata": {
        "tags": [],
        "id": "b667b32e-90d9-42b9-b5ff-456fbc04f522"
      },
      "outputs": [],
      "source": [
        "epochs = 10000000\n",
        "lr = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf456faa-52b1-4d01-873f-58b6de6756cc",
      "metadata": {
        "tags": [],
        "id": "bf456faa-52b1-4d01-873f-58b6de6756cc"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9016b5-ef57-44d5-a4e3-76482df7fe0a",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b9016b5-ef57-44d5-a4e3-76482df7fe0a",
        "outputId": "adbf3d86-4200-4933-daa6-3671e977738a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch152 Batch61: Discriminator Loss: 0.13231277465820312, Max: 1.9459129571914673, Generator Loss: 13.883942604064941, Min: 0.04864616319537163\n",
            "Epoch152 Batch62: Discriminator Loss: 0.014508675783872604, Max: 1.9459129571914673, Generator Loss: 7.28331995010376, Min: 0.04864616319537163\n",
            "Epoch152 Batch63: Discriminator Loss: 0.11136607080698013, Max: 1.9459129571914673, Generator Loss: 10.291675567626953, Min: 0.04864616319537163\n",
            "Epoch152 Batch64: Discriminator Loss: 0.04302675649523735, Max: 1.9459129571914673, Generator Loss: 13.677067756652832, Min: 0.04864616319537163\n",
            "Epoch152 Batch65: Discriminator Loss: 0.02079569175839424, Max: 1.9459129571914673, Generator Loss: 14.444747924804688, Min: 0.04864616319537163\n",
            "Epoch152 Batch66: Discriminator Loss: 0.22271686792373657, Max: 1.9459129571914673, Generator Loss: 8.138076782226562, Min: 0.04864616319537163\n",
            "Epoch152 Batch67: Discriminator Loss: 0.01504440139979124, Max: 1.9459129571914673, Generator Loss: 4.744025707244873, Min: 0.04864616319537163\n",
            "Epoch152 Batch68: Discriminator Loss: 0.05372932180762291, Max: 1.9459129571914673, Generator Loss: 16.92133903503418, Min: 0.04864616319537163\n",
            "Epoch152 Batch69: Discriminator Loss: 0.00718903262168169, Max: 1.9459129571914673, Generator Loss: 8.464559555053711, Min: 0.04864616319537163\n",
            "Epoch152 Batch70: Discriminator Loss: 0.11929137259721756, Max: 1.9459129571914673, Generator Loss: 21.35683250427246, Min: 0.04864616319537163\n",
            "Epoch152 Batch71: Discriminator Loss: 0.027604302391409874, Max: 1.9459129571914673, Generator Loss: 5.338354587554932, Min: 0.04864616319537163\n",
            "Epoch152 Batch72: Discriminator Loss: 0.12764333188533783, Max: 1.9459129571914673, Generator Loss: 16.436155319213867, Min: 0.04864616319537163\n",
            "Epoch152 Batch73: Discriminator Loss: 0.008215379901230335, Max: 1.9459129571914673, Generator Loss: 11.870240211486816, Min: 0.04864616319537163\n",
            "Epoch152 Batch74: Discriminator Loss: 0.05178322270512581, Max: 1.9459129571914673, Generator Loss: 4.7783894538879395, Min: 0.04864616319537163\n",
            "Epoch152 Batch75: Discriminator Loss: 0.035573430359363556, Max: 1.9459129571914673, Generator Loss: 2.6914193630218506, Min: 0.04864616319537163\n",
            "Epoch152 Batch76: Discriminator Loss: 0.012469070963561535, Max: 1.9459129571914673, Generator Loss: 15.66784381866455, Min: 0.04864616319537163\n",
            "Epoch152 Batch77: Discriminator Loss: 0.05233437567949295, Max: 1.9459129571914673, Generator Loss: 6.656796455383301, Min: 0.04864616319537163\n",
            "Epoch152 Batch78: Discriminator Loss: 0.06844154000282288, Max: 1.9459129571914673, Generator Loss: 4.474031448364258, Min: 0.04864616319537163\n",
            "Epoch152 Batch79: Discriminator Loss: 1.6941291093826294, Max: 1.9459129571914673, Generator Loss: 0.042699772864580154, Min: 0.042699772864580154\n",
            "Epoch152 Batch80: Discriminator Loss: 0.04155227541923523, Max: 1.9459129571914673, Generator Loss: 9.542985916137695, Min: 0.042699772864580154\n",
            "Epoch152 Batch81: Discriminator Loss: 0.059944044798612595, Max: 1.9459129571914673, Generator Loss: 12.261099815368652, Min: 0.042699772864580154\n",
            "Epoch152 Batch82: Discriminator Loss: 0.3252010643482208, Max: 1.9459129571914673, Generator Loss: 9.62870979309082, Min: 0.042699772864580154\n",
            "Epoch152 Batch83: Discriminator Loss: 0.8143916726112366, Max: 1.9459129571914673, Generator Loss: 0.7996804118156433, Min: 0.042699772864580154\n",
            "Epoch152 Batch84: Discriminator Loss: 0.11249507963657379, Max: 1.9459129571914673, Generator Loss: 7.956175327301025, Min: 0.042699772864580154\n",
            "Epoch152 Batch85: Discriminator Loss: 0.04789457097649574, Max: 1.9459129571914673, Generator Loss: 1.7041120529174805, Min: 0.042699772864580154\n",
            "Epoch152 Batch86: Discriminator Loss: 0.007602085825055838, Max: 1.9459129571914673, Generator Loss: 12.897258758544922, Min: 0.042699772864580154\n",
            "Epoch152 Batch87: Discriminator Loss: 0.07520273327827454, Max: 1.9459129571914673, Generator Loss: 2.382946252822876, Min: 0.042699772864580154\n",
            "Epoch152 Batch88: Discriminator Loss: 0.00905163586139679, Max: 1.9459129571914673, Generator Loss: 6.692230701446533, Min: 0.042699772864580154\n",
            "Epoch152 Batch89: Discriminator Loss: 0.022888457402586937, Max: 1.9459129571914673, Generator Loss: 5.9985737800598145, Min: 0.042699772864580154\n",
            "Epoch152 Batch90: Discriminator Loss: 0.027324678376317024, Max: 1.9459129571914673, Generator Loss: 11.393976211547852, Min: 0.042699772864580154\n",
            "Epoch152 Batch91: Discriminator Loss: 0.04399076849222183, Max: 1.9459129571914673, Generator Loss: 5.362771987915039, Min: 0.042699772864580154\n",
            "Epoch152 Batch92: Discriminator Loss: 0.04135045036673546, Max: 1.9459129571914673, Generator Loss: 10.527795791625977, Min: 0.042699772864580154\n",
            "Epoch152 Batch93: Discriminator Loss: 0.07695174962282181, Max: 1.9459129571914673, Generator Loss: 3.5899736881256104, Min: 0.042699772864580154\n",
            "Epoch152 Batch94: Discriminator Loss: 0.0261777825653553, Max: 1.9459129571914673, Generator Loss: 8.199995994567871, Min: 0.042699772864580154\n",
            "Epoch152 Batch95: Discriminator Loss: 0.01986568234860897, Max: 1.9459129571914673, Generator Loss: 4.261631011962891, Min: 0.042699772864580154\n",
            "Epoch152 Batch96: Discriminator Loss: 0.10334962606430054, Max: 1.9459129571914673, Generator Loss: 8.47929573059082, Min: 0.042699772864580154\n",
            "Epoch152 Batch97: Discriminator Loss: 0.320747435092926, Max: 1.9459129571914673, Generator Loss: 1.2061723470687866, Min: 0.042699772864580154\n",
            "Epoch152 Batch98: Discriminator Loss: 0.026418834924697876, Max: 1.9459129571914673, Generator Loss: 11.086669921875, Min: 0.042699772864580154\n",
            "Epoch152 Batch99: Discriminator Loss: 1.3387210369110107, Max: 1.9459129571914673, Generator Loss: 0.1278020143508911, Min: 0.042699772864580154\n",
            "Epoch152 Batch100: Discriminator Loss: 0.05664120241999626, Max: 1.9459129571914673, Generator Loss: 10.989558219909668, Min: 0.042699772864580154\n",
            "Epoch152 Batch101: Discriminator Loss: 0.012720249593257904, Max: 1.9459129571914673, Generator Loss: 7.891624927520752, Min: 0.042699772864580154\n",
            "Epoch152 Batch102: Discriminator Loss: 0.11366082727909088, Max: 1.9459129571914673, Generator Loss: 8.03380298614502, Min: 0.042699772864580154\n",
            "Epoch152 Batch103: Discriminator Loss: 0.15566487610340118, Max: 1.9459129571914673, Generator Loss: 6.748741626739502, Min: 0.042699772864580154\n",
            "Epoch152 Batch104: Discriminator Loss: 0.1335788071155548, Max: 1.9459129571914673, Generator Loss: 14.314010620117188, Min: 0.042699772864580154\n",
            "Epoch152 Batch105: Discriminator Loss: 0.17122691869735718, Max: 1.9459129571914673, Generator Loss: 2.202605962753296, Min: 0.042699772864580154\n",
            "Epoch152 Batch106: Discriminator Loss: 0.08617883175611496, Max: 1.9459129571914673, Generator Loss: 7.038640975952148, Min: 0.042699772864580154\n",
            "Epoch152 Batch107: Discriminator Loss: 0.15849152207374573, Max: 1.9459129571914673, Generator Loss: 3.142178535461426, Min: 0.042699772864580154\n",
            "Epoch152 Batch108: Discriminator Loss: 0.07394764572381973, Max: 1.9459129571914673, Generator Loss: 3.3731374740600586, Min: 0.042699772864580154\n",
            "Epoch152 Batch109: Discriminator Loss: 0.22852453589439392, Max: 1.9459129571914673, Generator Loss: 14.320474624633789, Min: 0.042699772864580154\n",
            "Epoch152 Batch110: Discriminator Loss: 0.29134994745254517, Max: 1.9459129571914673, Generator Loss: 1.4906480312347412, Min: 0.042699772864580154\n",
            "Epoch152 Batch111: Discriminator Loss: 0.12806746363639832, Max: 1.9459129571914673, Generator Loss: 14.595077514648438, Min: 0.042699772864580154\n"
          ]
        }
      ],
      "source": [
        "# Implementing GANs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "\n",
        "\n",
        "    batch_counter = 1\n",
        "    dis_loss_max = 0\n",
        "    gen_loss_min = 100\n",
        "\n",
        "    for real_img, real_label in dl:\n",
        "\n",
        "        real_img = real_img.to(device)\n",
        "        real_label = torch.tensor(1).float().to(device)\n",
        "\n",
        "        noise = generateNoise().to(device)\n",
        "        fake_img = generator(noise).to(device)\n",
        "        fake_label = torch.tensor(0).float().to(device)\n",
        "\n",
        "\n",
        "        #Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        loss_G = loss_fn(discriminator(fake_img).squeeze(), real_label)\n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "\n",
        "        #Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        fake_loss = loss_fn(discriminator(fake_img.detach()).squeeze(), fake_label)\n",
        "        real_loss = loss_fn(discriminator(real_img).mean().squeeze(), real_label)\n",
        "        loss_D = (fake_loss+real_loss)/2\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        gen_loss = loss_G.item()\n",
        "        dis_loss = loss_D.item()\n",
        "\n",
        "        if (gen_loss < gen_loss_min):\n",
        "            gen_loss_min = gen_loss\n",
        "\n",
        "        if (dis_loss > dis_loss_max):\n",
        "            dis_loss_max = dis_loss\n",
        "\n",
        "        print(f\"Epoch{epoch+1} Batch{batch_counter}: Discriminator Loss: {dis_loss}, Max: {dis_loss_max}, Generator Loss: {gen_loss}, Min: {gen_loss_min}\")\n",
        "        batch_counter += 1\n",
        "\n",
        "\n",
        "    if ((epoch+1)%1) == 0:\n",
        "        noise = generateNoise()\n",
        "        noise = noise.to(device)\n",
        "        fake_img = generator(noise)\n",
        "        fake_img = fake_img.to(device)\n",
        "        torchvision.utils.save_image(fake_img, 'generated.jpg')\n",
        "        plt.imshow(fake_img.cpu().squeeze().detach().permute(1, 2, 0))\n",
        "        plt.show()\n",
        "        torch.save(generator.state_dict(), 'generator.pt')\n",
        "        torch.save(discriminator.state_dict(), 'discriminator.pt')\n",
        "        #playsound('epoch10.mp3')\n",
        "\n",
        "    if gen_loss > 100:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sagemaker-distribution:Python",
      "language": "python",
      "name": "conda-env-sagemaker-distribution-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}